\typeout{ ====================================================================}
\typeout{ This is file euraxess-cv, created at Mon 14 May 11:45:34 2018 CEST  }
\typeout{ Andre Anjos <andre.anjos@idiap.ch>                                  }
\typeout{ ====================================================================}

\documentclass[11pt,a4paper,sans]{moderncv}
\usepackage[T1]{fontenc}

% styles: 'casual' (default), 'classic', 'oldstyle','banking'
\moderncvstyle{casual}

% colors: 'blue' (default), 'orange', 'green', 'red', 'purple', 'grey', 'black'
\moderncvcolor{blue}

% sorting=none will preserve the bib file order
\usepackage{csquotes}
\usepackage[backend=biber,maxbibnames=99,bibstyle=publist,plauthorhandling=highlight,marginyear=false,sorting=ddnt]{biblatex}
\setlength\bibitemsep{1.5\itemsep}
\plauthorname[André]{Anjos}

\addbibresource{fapesp.bib}

% adjust the page margins
\usepackage[margin=1.8cm,left=2.3cm]{geometry}
% if you want to change the width of the column with the dates
%\setlength{\hintscolumnwidth}{3cm}

% for the 'classic' style, if you want to force the width allocated to your
% name and avoid line breaks. be careful though, the length is normally
% calculated to avoid any overlap with your personal info; use this at your own
% typographical risks...

%\setlength{\makecvtitlenamewidth}{10cm}

% hyperref setup
\definecolor{linkcolor}{rgb}{0.1,0.1,0.4}

\AfterPreamble{\hypersetup{
  pdfauthor={Andre Anjos},
  pdftitle={Short Curriculum Vitae for Andre Anjos},
  pdfsubject={Short Curriculum Vitae for Andre Anjos},
  urlcolor=blue,
  colorlinks=true,    % false: boxed links; true: colored links
  linkcolor=linkcolor, % color of internal links (change box color with linkbordercolor)
  citecolor=linkcolor, % color of links to bibliography
  filecolor=linkcolor, % color of file links
  urlcolor=linkcolor   % color of external links
}}

% personal data
\firstname{André}
\familyname{Anjos}

% optional, remove / comment the line if not wanted
\address{Idiap Research Institute, rue Marconi 19, Centre du Parc, Martigny,
Switzerland, CH-1920}{}

% optional, remove / comment the line if not wanted
\phone{+41277217763}

% optional, remove / comment the line if not wanted
\email{andre.anjos@idiap.ch}

% optional, remove / comment the line if not wanted
\homepage{andreanjos.org}

% optional, remove / comment the line if not wanted
%\extrainfo{}

% optional, remove / comment the line if not wanted
%\quote{Some quote}

\begin{document}

\section{André Anjos -- Researcher, Head of Biosignal Processing Group}

André Anjos received his Ph.D. degree in signal processing from the
\href{https://www.ufrj.br/}{Federal University of Rio de Janeiro} in 2006. He
joined the \href{https://atlas.ch/}{ATLAS Experiment} at European Centre for
Particle Physics (\href{https://www.cern.ch/}{CERN}, Switzerland) from 2001
until 2010 where he worked in the development and deployment of the Trigger and
Data Acquisition systems that are nowadays powering the discovery of the Higgs
boson. During his time at \href{https://www.cern.ch/}{CERN}, André studied the
application of neural networks and statistical methods for particle recognition
at the trigger level and developed several software components still in use
today. In 2010, André joined the
\href{https://www.idiap.ch/en/scientific-research/biometrics-security-and-privacy}{Biometrics Security and Privacy Group} at the
\href{https://www.idiap.ch/en/scientific-research/biosignal-processing}{Idiap
Research Institute} where he worked with face and vein biometrics, presentation
attack detection, and reproducibility in research.  Since 2018 André heads the
\href{https://www.idiap.ch/en/scientific-research/biosignal-processing}{Biosignal
Processing Group} at Idiap. His current research interests include medical
applications, biometrics, image and signal processing, machine learning,
research reproducibility and open science.  Among André's open-source
contributions, one can cite \href{https://www.idiap.ch/software/bob}{Bob} and
the \href{https://www.idiap.ch/software/beat}{BEAT framework} for evaluation and
testing of machine learning systems. He teaches graduate-level
\href{http://edu.epfl.ch/coursebook/en/fundamentals-in-statistical-pattern-recognition-EE-612}{machine learning
courses} at the École Polytechnique Fédérale de Lausanne
(\href{https://www.epfl.ch/}{EPFL}) and master courses at Idiap's
\href{https://master-ai.ch/}{Master of AI}. He serves as reviewer for various
scientific journals in pattern recognition, machine learning, and image.

\input{education}

\section{Professional experience}

\cventry{2018--}{Researcher}{}{Idiap Research Institute, Martigny, Switzerland}{}{Head of the Biosignal Processing Group.}

\cventry{2014--2018}{Research Associate}{}{Idiap Research Institute, Martigny, Switzerland}{}{Research on Biometrics, Security and Computing.}

\cventry{2010--2013}{Post-doctoral Researcher}{}{Idiap Research Institute, Martigny, Switzerland}{}{Research on Biometrics, Security and Computing.}

\cventry{2004--2010}{Post-doctoral Researcher}{}{University of Wisconsin, Madison, USA}{}{Development and construction of the ATLAS Trigger and Data-Acquisition Systems, at \href{http://www.cern.ch}{CERN}, Switzerland.}

\defbibnote{myprenote}{Select publications which are relevant to the proposal
or significant to my research career.  For a full list of publications, please
consult \url{http://andreanjos.org/publications/}.}
\renewcommand{\refname}{Relevant Publications}
\nocite{*}
\printbibliography[prenote=myprenote]

\section{Current Research Grants}


\cventry{2019--2022}{EU H2020 "AI4EU"}{PI: Patrick Gatelier (Thales SA, FR)}{My role: Partner}{}{Budget: 20'000'000 EUR (419'621 CHF for Idiap)}

\cventry{2019--2022}{EU CHIST-ERA "LEARN-REAL"}{PI: Sylvain Calinon (Idiap, CH)}{My role: Co-PI}{}{Budget: 775'837 CHF (250'000 CHF for Idiap)}

\cventry{2018--2020}{EU CHIST-ERA "ALLIES"}{PI: Anthony Larcher (UNIMANS, FR)}{My role: Partner}{}{Budget: 496'620CHF (496'620 CHF for Idiap)}

\section{Current Student Supervisions}

\cventry{2020--2021}{Geoffrey Raposo}{Master thesis: \textit{Active tuberculosis exclusion from frontal chest X-ray images}}{}{}{}

\cventry{2019--2020}{Colombine Verzat}{Master thesis: \textit{Machine Learning for Adverse Event Detection in Latent Tuberculosis Infection Treatment}}{}{}{}

\section{Academic Indicators}

\cventry{1}{Peer-Reviewed Journals}{28}{}{}{}
\cventry{2}{Peer-Reviewed Conferences}{58}{}{}{}
\cventry{3}{Book Chapters}{9}{}{}{}
\cventry{4}{Master Thesis}{3}{}{}{}
\cventry{5}{Ph.D Thesis}{1}{}{}{}
\cventry{6}{Citations}{+19000 (h-index: 25)}{}{}{}
\cventry{7}{Patents}{3}{}{}{}

\section{Links}

\cventry{}{Google Scholar}{\url{https://scholar.google.com/citations?user=pAfLhMoAAAAJ}}{}{}{}{}
\cventry{}{ORCID}{\url{https://orcid.org/0000-0001-7248-4014}}{}{}{}{}

\section{Other information: Overview of Recent and Impactful Contributions}

\subsection{Semantic Segmentation for Medical Imaging}

Since the introduction of U-Nets in 2015, the field of medical image
segmentation has seen renewed interest bringing in a variety of fully
convolutional (deep) neural network (FCN) architectures for binary and
multi-class segmentation problems promising very attractive results, with
applications in computed tomography, retinography, and histopathology to cite a
few.  Despite the incredible progress, the lack of annotated images (due to
cost), and rigor in the comparison of trained models has led the community to
believe larger and more dense networks provide better results.  This is
particularly noticeable in ophtalmological images such as those from
bi-dimensional eye fundus photography (retinography).  While retinography is
not used for precision diagnosis, it remains cheap and very effective means for
mass screening.  Semantical segmentation of eye fundus structures plays a key
role in this process.

I tried to address these gaps in two different ways.  The
first~\cite{arxiv-2019} was to conduct and publish rigorous (open source,
reproducible) benchmarks with popular retinography datasets and
state-of-the-art FCN models in which we: i) showed that simple transformation
techniques like rescaling, padding and cropping of combined lower-resolution
source datasets to the resolution and spatial composition of a
higher-resolution target dataset can be a surprisingly effective way to improve
segmentation quality in unseen conditions; ii) we proposed a set of plots and
metrics that give additional insights into model performance and demonstrated
via tables and plots how to take advantage of that information, throwing a new
light over some published benchmarks.  We argue the performance of many
contributions available in literature is actually quite comparable within
standard deviation margins of each other, in spite of huge differences in the
number of parameters for different architectures.  Finally, we made our
findings reproducible, distributing code and documentation for future
researchers to build upon, in the hopes to inspire future work in the
field (\url{https://gitlab.idiap.ch/bob/bob.ip.binseg}).

In a second contribution~\cite{arxiv-2020} we propose that a minimalistic
version of a standard U-Net with 3 orders of magnitude less parameters,
carefully trained and rigorously evaluated, closely approximates the
state-of-the-art performance in vessel segmentation for retinography.  In
addition, we propose a simple extension, dubbed W-Net, by concatenating two
U-Nets together, which reaches outstanding performance on several popular
datasets, still using orders of magnitude less learnable weights than any
previously published approach.  This work also provide a very comprehensive
intra and cross-dataset performance analysis, involving up to 10 different
databases, including artery/vein multi-class semantic segmentation.


\subsection{Contributions to Computer Vision and Deep Learning in Biometrics}

I have actively worked in computer vision and deep learning (mostly) associated
to biometric recognition, with potential application to various other tasks.
Contributions range from the collection of datasets, the exploration of
different methods to address and assess biometric recognition vulnerabilities,
domain adaptation, and remote photoplethysmography.

While I have worked in various contributions, I highlight here key publications
from the past 5 years that may related to the proposal.

\begin{itemize}

    \item Domain Specific Units (Adaptation): In~\cite{tifs-2019}, we apply
        domain adaptation via dedicated Domain-Specific Units (DSU), with an
        application to Heterogeneous Face Recognition.  In this class of
        problems, one wants to recognize an individual across different
        spectral data,  based on the representation on a principal spectrum
        (e.g., visual).  It is a challenging task because multi-spectral data
        for covering large populations is rare, which in turn stymies the
        training of deep convolution-based architectures for this task.  We
        developed a mechanism to adapt the parameters of models pre-trained on
        large visual spectral face recognition datasets, which are readily
        available.  My contributions are directly related to core idea of this
        work.

  \item Anomaly Detection and Robustness: Biometric recognition systems are
      exposed to presentation attacks, and dectectors (PAD) for this purpose
      are required building blocks of thrustworthy systems.  Most PAD systems
      work discriminatively, trying to separate attacks from \textit{bona fide}
      presentations.  We showed this technique does not generalize well to
      unseen presentation attacks.  We explored, for the first time, alternate
      approaches by joint-modelling client identity as a way to calibrate PAD
      output scores~\cite{tifs-2015}, showing increased robustness to unseen
      attacks.  More recently, we also showed that solely modelling
      \textit{bona fide} presentations is also an effective way to increased
      PAD robustness~\cite{icb-2018}.  Finally, we showed that by adding
      heterogenous inputs to PAD systems can improve their
      robustness~\cite{tifs-2019-2} to achieve state-of-the-art performance,
      even to unseen conditions during training.

  \item Remote Photoplethysmography (rPPG):  This study~\cite{arxiv-2017-2}
      tackles the problem of reproducibility in rPPG. To date, most of the work
      published in this domain was assessed on privately-owned databases,
      making it difficult to evaluate proposed algorithms.   We introduded a
      new, publicly available dataset containing a relatively large number of
      subjects recorded under two different lighting conditions.  We then
      thoroughly benchmarked state-of-the-art algorithms using an unbiased
      experimental evaluation in various settings.  We showed that none of the
      selected algorithms is precise enough to be used in a real-world
      scenario.

\end{itemize}

This work was published as book chapters, international peer-reviewed
scientific journals (including articles at very high-impact factor journals)
and in peer-reviewed conference papers totalling a few thousand citations.
Accompanying software packages for such contributions were released publicly,
under an open-source license. For details and more links, please refer to the
applicant's Research Output.

\subsection{Reproducible Research}

We've been actively looking at the reproducibility of published work and how to
lower the entrance barrier of publication readers, converting them into engaged
users of methods we create.  We argue it is insufficient, in most cases, to
only publish software leading to results if original data remains inaccessible.
In particular~\cite{acmmm-2012}, we note that reproducibility should imply
in the following characteristics: repeatability, shareability, extensibility
and stability, which is not guaranteed by most published material to date.  We
proposed an open-source software suite called Bob
(\url{https://www.idiap.ch/software/bob}) that possesses such characteristics,
demonstrating its flexibility to various tasks including Medical Image
Segmentation (\url{https://gitlab.idiap.ch/bob/bob.ip.binseg}), Remote
Photoplethysmography (\url{https://gitlab.idiap.ch/bob/bob.rppg.base}), and
Biometric Person Recognition (\url{https://gitlab.idiap.ch/bob/bob.bio.base}).

From another perspective, there are legitimate cases in which raw data leading
to research conclusions cannot be published.  Furthermore, in a growing number
of use-cases, the availability of both software does not translate to an
accessible reproducibility scenario.  The user, for example, may not have the
necessary equipment to perform the analysis.  To bridge this gap, we built an
open platform for research (\url{https://www.beat-eu.org/platform}) in
computational sciences related to pattern recognition and machine learning, to
help on the development, reproducibility and certification of results obtained
in the field~\cite{icml-2017-1}.  The BEAT platform is distributed under an
open-source license (\url{https://www.idiap.ch/software/beat/}).

Both projects are still active and support past and future work at Idiap and
beyond.  We conducted (and will continue doing) lectures to both master and
graduate students about reproducibility in data science (refer to the
applicant's CV under the section "Teaching" for details).  There are currently
$\sim$180 \textit{direct} citations to publications about Bob and BEAT core
frameworks.

\end{document}

\typeout{ *************** End of file cv *************** }
